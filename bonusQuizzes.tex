\subsection*{Quizzes - Computations}
\tsIdea{Basis of a plane and related subspaces}{
    \[
        \text{Plane } P = \{x \in \mathbb{R}^3 : 6x_1 - x_2 + 5x_3 = 0\}
    \]
    \textbf{1. Basis for } P
    \\
    Solve for \(x_2\):
    \[
        x_2 = 6x_1 + 5x_3
    \]
    Let \(x_1=s,\; x_3=t\):
    \[
        (x_1,x_2,x_3) = (s,6s+5t,t)
        = s(1,6,0) + t(0,5,1)
    \]
    \[
        \boxed{\mathcal B_P = \{(1,6,0),(0,5,1)\}}
    \]
    \textbf{2. Intersection with } $\operatorname{span}\{e_1,e_2\}$
    \\
    \[
        x_3 = 0 \Rightarrow 6x_1 - x_2 = 0 \Rightarrow x_2 = 6x_1
    \]
    \[
        (x_1,x_2,x_3) = s(1,6,0)
    \]
    \[
        \boxed{\mathcal B_{P \cap \operatorname{span}\{e_1,e_2\}} = \{(1,6,0)\}}
    \]
    \textbf{3. Perpendicular vectors to } P
    \\
    Normal vector from plane equation:
    \[
        \mathbf n = (6,-1,5)
    \]
    \[
        P^\perp = \operatorname{span}\{(6,-1,5)\}
    \]
    \[
        \boxed{\mathcal B_{P^\perp} = \{(6,-1,5)\}}
    \]
}
\tsIdea{Compute bases for orthogonal spaces}{
    \textbf{Finding } $S^\perp \subset \mathbb{R}^3$ \textbf{ when } $S=\mathrm{span}\{v\}$
    \\
    Let $v=(a,b,c)\neq 0$ and $S=\mathrm{span}\{v\}$.
    Then
    \[
        S^\perp=\{u\in\mathbb{R}^3 : u\cdot v=0\}.
    \]
    Let $u=(x,y,z)$. Orthogonality gives
    \[
        ax+by+cz=0.
    \]
    Solve for one variable and parametrize.
    Choose convenient values for the free variables to obtain
    two linearly independent solutions.
    \\
    These two vectors form a basis for $S^\perp$.
    \\\textbf{Example: } v=(-6,-9,7)
    \[
        -6x-9y+7z=0
    \]
    Choose $(y,z)=(2,0)$ and $(0,6)$:
    \[
        u_1=(-3,2,0), \quad u_2=(7,0,6).
    \]
}
\tsIdea{Calculating four fundamental subspaces}{
    \textbf{Four Fundamental Subspaces}
    \\
    Let $A\in\mathbb{R}^{m\times n}$ with $\operatorname{rank}(A)=r$.
    \[
        \begin{aligned}
            \dim \mathcal{C}(A)   & = r   \\
            \dim \mathcal{C}(A^T) & = r   \\
            \dim \mathcal{N}(A)   & = n-r \\
            \dim \mathcal{N}(A^T) & = m-r
        \end{aligned}
    \]
    \[
        \mathcal{C}(A)\perp\mathcal{N}(A^T),\quad
        \mathcal{C}(A^T)\perp\mathcal{N}(A)
    \]
}
\tsIdea{Pseudoinverse of diagonal matrix}{
    Diagonal matrix \(\Rightarrow\) invert nonzero diagonals, keep zeros.
}
\newline
\tsIdea{Diagonalization of a symmetric matrix}{
    \textbf{Diagonalization of a symmetric matrix (example)}
    Let
    \[
        A=\begin{pmatrix}2&1\\[2pt]1&2\end{pmatrix}.
    \]
    Characteristic polynomial:
    \[
        p_A(\lambda)=\det(A-\lambda I)
        =(2-\lambda)^2-1
        =\lambda^2-4\lambda+3
        =(\lambda-3)(\lambda-1).
    \]
    Eigenvalues:
    \[
        \lambda_1=3,\quad \lambda_2=1.
    \]
    Eigenvectors:
    \[
        \lambda_1=3:\ v_1=\begin{pmatrix}1\\1\end{pmatrix},\qquad
        \lambda_2=1:\ v_2=\begin{pmatrix}1\\-1\end{pmatrix}.
    \]
    The eigenvectors are orthogonal:
    \[
        v_1\cdot v_2=0.
    \]
    Define
    \[
        V=\begin{pmatrix}1&1\\[2pt]1&-1\end{pmatrix},
        \qquad
        \Lambda=\begin{pmatrix}3&0\\[2pt]0&1\end{pmatrix}.
    \]
    Then
    \[
        A=V\Lambda V^{-1},
        \qquad
        V^{-1}=(V^TV)^{-1}V^T=\tfrac12 V^T.
    \]
    \textbf{Remark:}
    The order of eigenvalues on the diagonal of $\Lambda$ is arbitrary.
    Reordering eigenvalues requires the same reordering of eigenvectors.
}
\newline
\tsIdea{Computing singular values}{
    \textbf{Computing singular values}
    \\
    For a matrix $A\in\mathbb{R}^{m\times n}$, the singular values are
    \[
        \sigma_i=\sqrt{\lambda_i},
    \]
    where $\lambda_i$ are the eigenvalues of $A^TA$.
    \\
    \textbf{Example:}
    \[
        A=\begin{pmatrix}3&0\\4&0\end{pmatrix}.
    \]
    Compute
    \[
        A^TA=
        \begin{pmatrix}3&4\\0&0\end{pmatrix}
        \begin{pmatrix}3&0\\4&0\end{pmatrix}
        =
        \begin{pmatrix}25&0\\0&0\end{pmatrix}.
    \]
    Eigenvalues of $A^TA$:
    \[
        \lambda_1=25,\quad \lambda_2=0.
    \]
    Singular values:
    \[
        \sigma_1=\sqrt{25}=5,\quad \sigma_2=\sqrt{0}=0.
    \]
    \[\boxed
        {
            \text{Singular values of }A=\sqrt{\text{eigenvalues of }A^TA}.
        }
    \]
}
\tsIdea{Why determinant expansion along a row/column works}{
    \textbf{Why determinant expansion along a row/column works}
    \\
    For any $n\times n$ matrix $M=(m_{ij})$, the determinant can be expanded
    along any row $i$ or any column $j$ (Laplace expansion):
    \\
    \[
        \det(M)=\sum_{j=1}^n m_{ij}C_{ij}
        \quad\text{or}\quad
        \det(M)=\sum_{i=1}^n m_{ij}C_{ij},
    \]
    where
    \[
        C_{ij}=(-1)^{i+j}\det(M_{ij})
    \]
    is the cofactor, and $M_{ij}$ is obtained by deleting row $i$ and column $j$.
    \\ \\
    \textbf{Example:}
    \[
        \lambda I-A=
        \begin{pmatrix}
            \lambda-\frac92 & 0       & -\frac12        \\
            0               & \lambda & 0               \\
            -\frac12        & 0       & \lambda-\frac92
        \end{pmatrix}.
    \]
    Expanding along row 2:
    \[
        \det(\lambda I-A)
        =0\cdot C_{21}
        +\lambda\cdot C_{22}
        +0\cdot C_{23}
        =\lambda\,C_{22}.
    \]
    \textbf{Cofactor computation:}
    \[
        C_{22}=(-1)^{2+2}
        \det
        \begin{pmatrix}
            \lambda-\frac92 & -\frac12        \\
            -\frac12        & \lambda-\frac92
        \end{pmatrix}.
    \]
    Since $(-1)^{2+2}=(-1)^4=1$, we obtain
    \[
        C_{22}=
        \det
        \begin{pmatrix}
            \lambda-\frac92 & -\frac12        \\
            -\frac12        & \lambda-\frac92
        \end{pmatrix}.
    \]
    Therefore,
    \[
        \det(\lambda I-A)
        =
        \lambda
        \det
        \begin{pmatrix}
            \lambda-\frac92 & -\frac12        \\
            -\frac12        & \lambda-\frac92
        \end{pmatrix}.
    \]
    \\
    \textbf{Conclusion:}
    Expanding along rows or columns with many zeros is always valid and
    simplifies determinant computations.
}
\newline
\tsIdea{Fast computation of singular values (symmetric case)}{
    \textbf{Fast computation of singular values (symmetric case)}
    Let $A\in\mathbb{R}^{n\times n}$.
    \textbf{Key fact:}
    If $A=A^T$ (i.e.\ $A$ is symmetric), then
    \[
        \boxed{\sigma_i(A)=|\lambda_i(A)|},
    \]
    where $\lambda_i(A)$ are the eigenvalues of $A$.
    \textbf{Reason:}
    Singular values are defined by
    \[
        \sigma_i(A)=\sqrt{\lambda_i(A^TA)}.
    \]
    If $A=A^T$, then
    \[
        A^TA=A^2,
    \]
    and eigenvalues of $A^2$ are $\lambda_i(A)^2$.
    Hence
    \[
        \sigma_i(A)=\sqrt{\lambda_i(A)^2}=|\lambda_i(A)|.
    \]
    \textbf{Example:}
    \[
        A=
        \begin{pmatrix}
            \frac92 & 0 & \frac12 \\
            0       & 0 & 0       \\
            \frac12 & 0 & \frac92
        \end{pmatrix}
        \quad\text{(symmetric)}.
    \]
    Eigenvalues:
    \[
        \lambda(A)=\{5,\ 4,\ 0\}.
    \]
    Singular values:
    \[
        \boxed{\sigma(A)=\{5,\ 4,\ 0\}}.
    \]
    \textbf{General fallback (always works):}
    If $A$ is not symmetric,
    \[
        \text{compute eigenvalues of } A^TA \text{ and take square roots.}
    \]
    \textbf{Remember:}
    \[
        \boxed{A=A^T \;\Rightarrow\; \sigma_i=|\lambda_i|.}
    \]
}
\newline
\tsIdea{Similar (2x2) matrices}{
    Similar (2x2) matrices always have the same eigenvalues. It is the fast way to check if matrices are similar.
}
\newline
\tsIdea{Norm of a vector}{
    Norm (2) of a vector:
    \[
        \boxed{
            \|\mathbf{v}\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}
        }
    \]
}
\newline
\tsIdea{Characteristic polynomial \& eigenvalues \& eigenvectors}{
    We can find Characteristic polynomial via:
    \[
        \boxed{\det(\lambda I - A) = (-1)^n \det(A - \lambda I)}
    \]
    \(\bullet\) We can find corresponding eigenvectors $\mathbf{v}$ to eigenvalues $\lambda$ via:
    \[
        \boxed{(A - \lambda I)\mathbf{v} = 0}
    \]
}
\newline
\tsIdea{Distance between vector and its projection}{
    Distance between vector and its projection is:
    \[
        \boxed{\| \text{vector} - \text{projection} \| = \| \mathbf{v} - \operatorname{proj}_\textit{S}(\mathbf{v}) \|}
    \]
}
\newline
\tsIdea{Angle between vectors}{
    Angle between vectors:
    \[
        \boxed{
            \cos(\theta) = \left( \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} \right)
        }
    \]
}
\newline
\tsIdea{When \(\textit{T}\) is linear transformation}{
    When \(\textit{T}\) is linear transformation, then:
    \[
        \boxed{2 \textit{T}(\mathbf{x}) + 3 \textit{T}(\mathbf{y}) = \textit{T}(2\mathbf{x} + 3\mathbf{y})}
    \]
    This might simplify some calculations a lot.
}
\newline
\tsIdea{Inverse of \(2 \times 2\) matrix}{
    Inverse of \(2 \times 2\) matrix \(A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}\) with \(\det(A) \neq 0\):
    \[
        \boxed{
            A^{-1} = \frac{1}{ad-bc}\begin{pmatrix}
                d  & -b
                \\
                -c & a
            \end{pmatrix}
        }
    \]
}
% From correction of parts with multiple choice questions
\tsIdea{Determining when matrix returns to identity}{
    If $A^{m}=I$ and $A^{n}=I$, then
    \[
        A^{\gcd(m,n)}=I.
    \]
    Proof sketch to template: \\
    Let $d=\gcd(m,n)$. Then there exist integers $x,y$ with $d=xm+yn$ (B\'ezout).
    Assuming $A$ is invertible (true in any group; for matrices this means $A\in GL$),
    \[
        A^{d}=A^{xm+yn}=(A^{m})^{x}(A^{n})^{y}=I^{x}I^{y}=I.
    \]
    \textbf{When can you conclude $A=I$?}
    You can conclude $A=I$ if $\gcd(m,n)=1$, because then $A^{1}=A=I$.
    Otherwise, you can only conclude that the order of $A$ divides $\gcd(m,n)$ (i.e., $A$ is a root of unity of that exponent).
}
\newline
\tsIdea{Pairwise linear independence}{
    Pairwise independence is weaker than (joint) linear independence. \(\Rightarrow\) Linear independence is a global property: checking vectors two at a time is not enough. Collectively, they might still fail independence.
    \\
    Geometrically in \(\mathbb{R}^2\): you can have infinitely many vectors that are pairwise non-collinear, but at most two vectors can be linearly independent.
}
\newline
\tsIdea{Complex expression and geometric Interpretation}{
    Using $z\bar z = |z|^2$, the condition reduces to $x^2+y^2=1$, which describes
    the unit circle.
}
\newline
\tsIdea{SVD of rank-1 matrix}{
SVD of rank-1 matrix:
\[A = \sigma u v^\top\]
Where:
\begin{enumerate}[noitemsep, topsep=-1px]
    \item \(\sigma\) is only on non-zero singular value
    \item \(u\) is a unit column vector (left singular vector)
    \item \(v\) is a unit column vector (right singular vector)
\end{enumerate}
More \textit{specifically}, if
\[
    A = x\,y^T
\]
with \(x \in \mathbb{R}^m,\; y \in \mathbb{R}^n\), then
\[
    A = \sigma\, u\, v^T
\]
where
\[
    \sigma = \|x\|\,\|y\|,\quad
    u = \frac{x}{\|x\|},\quad
    v = \frac{y}{\|y\|}
\]
\textbf{Example:}
\[
    A =
    \begin{bmatrix}
        \sqrt{6} \\ 3\\ -1
    \end{bmatrix}
    \begin{bmatrix}
        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
    \end{bmatrix}
\]
\[
    \|x\| = \sqrt{6+9+1}=4,\quad \|y\|=1
\]
\[
    A =
    \begin{bmatrix}
        \frac{\sqrt{6}}{4} \\ \frac{3}{4}\\ -\frac{1}{4}
    \end{bmatrix}
    [4]
    \begin{bmatrix}
        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
    \end{bmatrix}
\]
\[
    \boxed{\text{Unique non-zero singular value } \sigma = 4}
\]
}
\tsIdea{SVD validity check}{
    To verify whether a proposed factorization $A = U\Sigma V^\top$ is a valid SVD,
    it suffices to check the following:
    \begin{itemize}[noitemsep, topsep=-1px]
        \item \textbf{Orthogonality:} $U$ (and $V$) has orthonormal columns, i.e.\ $U^\top U = I$ and $V^\top V = I$;
        \item \textbf{Singular values:} $\Sigma$ is diagonal with non-negative entries;
        \item \textbf{Dimensions:} if $A\in\mathbb{R}^{m\times n}$, then $\Sigma\in\mathbb{R}^{m\times n}$.
    \end{itemize}
}
\tsIdea{Invertible matrix and EW}{
    A matrix is invertible iff $0$ is not an eigenvalue.
    \(\tsPoint\) $A=A^T
        \Rightarrow A \text{ has real eigenvalues}
        \Rightarrow \lambda+i\neq0
        \Rightarrow \det(A+iI)\neq0
        \Rightarrow A+iI \text{ is invertible}
    $
}