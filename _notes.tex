\subsection*{Notes and ideas}
\tsIdea{Pythagorean theorem}
{If two vectors are orthogonal, then their squared lengths add:
    \\
    \(\|\tsV{x} + \tsV{y}\|^2 = \tsV{x}^\top \tsV{x} + 2\underbrace{\tsV{x}^\top \tsV{y}}_{=0} + \tsV{y}^\top \tsV{y} = \|\tsV{x}\|^2 + \|\tsV{y}\|^2\)
}
\newline
\tsIdea{Linearity Proof}
{To prove linearity insert arbitrary
    $\tsV{v}, \tsV{w} \in \mathbb{R}$ and $\alpha \in \tsS{R}$ to $f(\tsV{v} + \alpha\tsV{w})$
}
\newline
\tsIdea{Non-trivial nullspace and solutions}
{When  \(m < n\) then there exists a nontrivial null space, which prevents uniqueness of a solution. Let \(A \in \tsS{R}^{m \times n}\), then there exists \(\tsV{x} \neq 0\) but \(A\tsV{x} = 0\):
    \\
    \(\text{rank}(A) \leq m < n \Rightarrow dim(N(A))=n-\text{rank}(A \geq 1)\)
}
\newline
\tsIdea{Linear dependence}
{The vectors $v_1, v_2, v_3$ are \emph{linearly dependent} if there exist scalars
    $a_1, a_2, a_3$, not all zero, such that
    \(
    a_1 v_1 + a_2 v_2 + a_3 v_3 = \mathbf{0}.
    \)
}
\newline
\tsIdea{Invertible matrix properties}
{\(A\) is invertible
    \\
    \(\Leftrightarrow\) \(A\tsV{x}=\tsVec{b}\) has a unique solution for every \(\tsVec{b}\)
    \\
    \(\Leftrightarrow\) \(N(A) = \tsB{0}\)
    \\
    \(\Leftrightarrow\) \(\text{rank}(A) = n\)
    \\
    \(\Leftrightarrow\) columns of \(A\) are lin. independent
}
\newline
\tsIdea{Homogenous system solutions}
{A homogenous system \(A\tsVec{x} = 0\) has either one solution \(x = 0\) or infinitely many solutions: all scaled \(\alpha \tsV{x}\).
}
\newline
\tsIdea{Square a sum}{
    \[\sum_{i=1}^{n} \lambda_i)^2 = (\sum_{i=1}^{n} \lambda_i)(\sum_{j=1}^{n} \lambda_j)
        \\
        = \sum_{i=1}^{n}\sum_{j=1}^{n}\lambda_i\lambda_j = \sum_{i=1}^{n}\lambda_i^2 + \sum_{i\neq j}^{} \lambda_i \lambda_j\]
    \\
    That is why \(\text{Tr}(A^2) \leq \text{Tr}(A)^2\). When all eigenvalues are zero or at most one is not zero, then this becomes equal.
}
\newline
\tsIdea{Dot product concept}{Dot product tells you how much two vectors point in the same direction.
    \begin{itemize}[noitemsep, topsep=0pt]
        \item $u^T w < 0$: pointing opposite to $w$
        \item $u^T w = 0$: perpendicular to $w$
        \item $u^T w > 0$: pointing partly in the same direction as $w$
    \end{itemize}
}
\tsIdea{Prove the dimension of a subspace}{Write the general matrix, apply the constraints, deconstruct into linear combination of independent variables, prove that they span the space, show that they are linearly independent.}
\newline
\tsIdea{Singular Values inversion}{Since
    \(
    \sigma_1 \ge \cdots \ge \sigma_n > 0,
    \)
    it follows that
    \(
    \frac{1}{\sigma_1} \le \cdots \le \frac{1}{\sigma_n},
    \)
    and therefore, when ordered decreasingly, the singular values of \( A^{-1} \) are
    \(
    \frac{1}{\sigma_n}, \ldots, \frac{1}{\sigma_1}.
    \)
}
\newline
\tsIdea{Invertible matrix singular values}{Invertible matrix singular values are strictly positive.}
\newline
\tsIdea{Creative Gauchy-Schwarz Inequality}{To prove
    \[
        \sum_{i=1}^n \frac{a_i^2}{b_i}
        \;\ge\;
        \frac{\left(\sum_{i=1}^n a_i\right)^2}{\sum_{i=1}^n b_i},
    \]
    apply the Cauchy-Schwarz inequality to the vectors
    \[
        \left(
        \frac{a_1}{\sqrt{b_1}},
        \dots,
        \frac{a_n}{\sqrt{b_n}}
        \right)
        \quad\text{and}\quad
        \left(
        \sqrt{b_1},
        \dots,
        \sqrt{b_n}
        \right).
    \]
    Their dot product is
    \[
        \sum_{i=1}^n a_i,
    \]
    and their squared norms are
    \[
        \sum_{i=1}^n \frac{a_i^2}{b_i}
        \quad\text{and}\quad
        \sum_{i=1}^n b_i.
    \]
    By Cauchy-Schwarz,
    \\
    \[
        \left(\sum_{i=1}^n a_i\right)^2
        \le
        \left(\sum_{i=1}^n \frac{a_i^2}{b_i}\right)
        \left(\sum_{i=1}^n b_i\right),
    \]
    \\
    which rearranges by division with a sum of \(b\) to the desired inequality.
}
% \tsIdea{Matrix Representation of a Linear Map}{
% \[
%     f:\mathbb R^2 \to M_{2\times2}(\mathbb R), \qquad
%     f(a,b)=
%     \begin{pmatrix}
%         a & -b \\
%         b & a
%     \end{pmatrix}
% \]

% \[
%     \mathcal B_1=
%     \left\{
%     \begin{pmatrix}1&0\\0&-1\end{pmatrix},
%     \begin{pmatrix}0&-1\\1&0\end{pmatrix},
%     \begin{pmatrix}1&0\\0&0\end{pmatrix}
%     \begin{pmatrix}0&0\\0&1\end{pmatrix}
%     \right\}
% \]

% \[
%     \mathcal B_1=\{B_1,B_2,B_3,B_4\}
% \]

% \[
%     f(a,b)=aB_1+bB_2+0B_3+2aB_4
% \]

% \[
%     [f(a,b)]_{\mathcal B_1}
%     =
%     \begin{pmatrix}
%         a \\
%         b \\
%         0 \\
%         2a
%     \end{pmatrix}
% \]

% \[
%     F\begin{pmatrix}a\\b\end{pmatrix}
%     =
%     [f(a,b)]_{\mathcal B_1},
%     \qquad
%     F=
%     \begin{pmatrix}
%         1 & 0 \\
%         0 & 1 \\
%         0 & 0 \\
%         2 & 0
%     \end{pmatrix}
% \]

% \[
%     \text{Columns of }F = [f(e_1)]_{\mathcal B_1},\ [f(e_2)]_{\mathcal B_1}
% \]
% }
\tsIdea{Why Tr\((A) = \sum_{i=1}^{n}\lambda_i \)}{
    Eigenvalues are the roots of the characteristic polynomial.
    \\
    Let $A\in\mathbb{R}^{n\times n}$ with eigenvalues $\lambda_1,\dots,\lambda_n$.
    The characteristic polynomial is
    \[
        \chi_A(z)
        = (-1)^n\det(A-zI)
        = \det(zI-A)
        = \prod_{i=1}^n (z-\lambda_i).
    \]

    \textbf{Determinant.}
    Evaluating at $z=0$ gives
    \[
        \det(A)=\prod_{i=1}^n \lambda_i.
    \]

    \textbf{Trace.}
    Expanding $\chi_A(z)$,
    \[
        \prod_{i=1}^n (z-\lambda_i)
        = z^n - \Big(\sum_{i=1}^n \lambda_i\Big) z^{n-1} + \cdots .
    \]
    Hence the coefficient of $z^{n-1}$ is $-\sum_{i=1}^n \lambda_i$.

    On the other hand, using the determinant expansion,
    \[
        \det(zI-A)
        = (z-A_{11})(z-A_{22})\cdots(z-A_{nn}) + \cdots ,
    \]
    where the remaining terms correspond to non-identity permutations and have
    degree at most $z^{n-2}$. Therefore, the coefficient of $z^{n-1}$ is
    \[
        -\sum_{i=1}^n A_{ii} = -\operatorname{Tr}(A).
    \]

    Comparing coefficients yields
    \[
        \operatorname{Tr}(A)=\sum_{i=1}^n \lambda_i.
    \]
}
\subsection{Quiz}
\tsIdea{Determining when matrix returns to identity}{
    If $A^{m}=I$ and $A^{n}=I$, then
    \[
        A^{\gcd(m,n)}=I.
    \]
    Proof sketch to template: \\
    Let $d=\gcd(m,n)$. Then there exist integers $x,y$ with $d=xm+yn$ (B\'ezout).
    Assuming $A$ is invertible (true in any group; for matrices this means $A\in GL$),
    \[
        A^{d}=A^{xm+yn}=(A^{m})^{x}(A^{n})^{y}=I^{x}I^{y}=I.
    \]
    \textbf{When can you conclude $A=I$?}
    You can conclude $A=I$ if $\gcd(m,n)=1$, because then $A^{1}=A=I$.
    Otherwise, you can only conclude that the order of $A$ divides $\gcd(m,n)$ (i.e., $A$ is a root of unity of that exponent).
}
\newline
\tsIdea{Pairwise linear independence}{
    Pairwise independence is weaker than (joint) linear independence. \(\Rightarrow\) Linear independence is a global property: checking vectors two at a time is not enough. Collectively, they might still fail independence.
    \\
    Geometrically in \(\mathbb{R}^2\): you can have infinitely many vectors that are pairwise non-collinear, but at most two vectors can be linearly independent.
}
\newline
\tsIdea{Linear Dependence Criterion via Dimension}{
    In a vector space of dimension $n$, any set of more than $n$ vectors is linearly dependent.
    More formally:
    \(\tsPoint\) Let $V$ be a vector space with $\dim V = n$.
    Then any set of $n+1$ (or more) vectors in $V$ is linearly dependent.
}
\newline
\tsIdea{Complex expression and geometric Interpretation}{
    Using $z\bar z = |z|^2$, the condition reduces to $x^2+y^2=1$, which describes
    the unit circle.
}
\newline
\tsIdea{SVD validity check}{
    To verify whether a proposed factorization $A = U\Sigma V^\top$ is a valid SVD,
    it suffices to check the following:
    \begin{itemize}[noitemsep, topsep=-1px]
        \item \textbf{Orthogonality:} $U$ (and $V$) has orthonormal columns, i.e.\ $U^\top U = I$ and $V^\top V = I$;
        \item \textbf{Singular values:} $\Sigma$ is diagonal with non-negative entries;
        \item \textbf{Dimensions:} if $A\in\mathbb{R}^{m\times n}$, then $\Sigma\in\mathbb{R}^{m\times n}$.
    \end{itemize}
}
\tsIdea{Invertible matrix and EW}{
    A matrix is invertible iff $0$ is not an eigenvalue.
    \(\tsPoint\) $A=A^T
        \Rightarrow A \text{ has real eigenvalues}
        \Rightarrow \lambda+i\neq0
        \Rightarrow \det(A+iI)\neq0
        \Rightarrow \det(A+iI) \text{ is invertible}
    $
}
